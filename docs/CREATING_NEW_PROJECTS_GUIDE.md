# Creating New Projects Based on SynthForge.AI

**Document Version:** 1.0  
**Date:** February 14, 2026  
**Purpose:** Step-by-step guide for creating new multi-agent projects using SynthForge.AI as a template

---

## Table of Contents

1. [Project Template Overview](#1-project-template-overview)
2. [Prerequisites](#2-prerequisites)
3. [Quick Start](#3-quick-start)
4. [Customizing Agent Architecture](#4-customizing-agent-architecture)
5. [Adapting to Different Domains](#5-adapting-to-different-domains)
6. [Example Projects](#6-example-projects)
7. [Scaffolding Tools](#7-scaffolding-tools)
8. [Testing Your New Project](#8-testing-your-new-project)

---

## 1. Project Template Overview

SynthForge.AI provides a robust template for building multi-agent systems with:

### Core Components You Can Reuse

```
SynthForge.AI Template
‚îú‚îÄ‚îÄ Agent Architecture Pattern
‚îÇ   ‚îú‚îÄ‚îÄ Base agent class with lifecycle management
‚îÇ   ‚îú‚îÄ‚îÄ Factory pattern for agent creation
‚îÇ   ‚îú‚îÄ‚îÄ Typed input/output with Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ Context manager pattern (__aenter__/__aexit__)
‚îÇ
‚îú‚îÄ‚îÄ Instruction System
‚îÇ   ‚îú‚îÄ‚îÄ YAML/Markdown-based instructions
‚îÇ   ‚îú‚îÄ‚îÄ Includes mechanism for shared principles
‚îÇ   ‚îú‚îÄ‚îÄ Severity framework (BLOCKING/HIGH/MEDIUM/LOW)
‚îÇ   ‚îî‚îÄ‚îÄ Dynamic tool selection guidance
‚îÇ
‚îú‚îÄ‚îÄ Tool System
‚îÇ   ‚îú‚îÄ‚îÄ Unified tool interface
‚îÇ   ‚îú‚îÄ‚îÄ Web search, GitHub search, file search
‚îÇ   ‚îú‚îÄ‚îÄ Tool registry pattern
‚îÇ   ‚îî‚îÄ‚îÄ Tool selection strategies
‚îÇ
‚îú‚îÄ‚îÄ Orchestration Patterns
‚îÇ   ‚îú‚îÄ‚îÄ Sequential pipeline
‚îÇ   ‚îú‚îÄ‚îÄ Parallel execution
‚îÇ   ‚îú‚îÄ‚îÄ Event-driven workflows
‚îÇ   ‚îú‚îÄ‚îÄ Feedback loops
‚îÇ   ‚îî‚îÄ‚îÄ State management
‚îÇ
‚îî‚îÄ‚îÄ GitHub Integration
    ‚îú‚îÄ‚îÄ GitHub Actions workflows
    ‚îú‚îÄ‚îÄ PR comment handlers
    ‚îú‚îÄ‚îÄ Artifact-based state
    ‚îú‚îÄ‚îÄ Status checks
    ‚îî‚îÄ‚îÄ Issue templates
```

### What You Need to Customize

1. **Domain Logic**: Replace Azure-specific logic with your domain
2. **Agent Purposes**: Define what each agent does in your domain
3. **Data Models**: Create Pydantic models for your data
4. **Instructions**: Write agent instructions for your use case
5. **Tools**: Add domain-specific tools if needed

---

## 2. Prerequisites

### 2.1 Technical Requirements

```bash
# Required
- Python 3.11+
- Git
- GitHub CLI (gh)
- GitHub Copilot access (for Copilot version)

# Recommended
- Docker (for containerization)
- VS Code with Copilot extension
- PostgreSQL (for production state management)
```

### 2.2 Knowledge Requirements

- Python async programming
- Pydantic data models
- GitHub Actions
- Basic understanding of multi-agent systems

### 2.3 Access Requirements

- GitHub organization or personal account
- GitHub Copilot license (if using Copilot version)
- Repository creation permissions

---

## 3. Quick Start

### 3.1 Clone the Template

```bash
# Option 1: Fork the repository
gh repo fork SamRakaba/SynthForge.AI --clone

# Option 2: Use as template
gh repo create MyOrg/MyAgentProject \
  --template SamRakaba/SynthForge.AI \
  --public \
  --clone

cd MyAgentProject
```

### 3.2 Set Up Your Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp .env.example .env
# Edit .env with your settings
```

### 3.3 Run the Scaffolding Tool

```bash
# Interactive project setup
python scripts/scaffold_project.py

# You'll be prompted for:
# - Project name
# - Project description
# - Number of agents
# - Agent names and purposes
# - IaC format (if applicable)
# - Domain (Azure, AWS, GCP, Kubernetes, etc.)
```

### 3.4 Generated Project Structure

```
MyAgentProject/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îú‚îÄ‚îÄ agents/               # Agent instruction files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent1.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent2.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent3.md
‚îÇ   ‚îú‚îÄ‚îÄ workflows/            # GitHub Actions workflows
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phase1.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ phase2.yml
‚îÇ   ‚îî‚îÄ‚îÄ copilot/              # Copilot configurations
‚îÇ       ‚îî‚îÄ‚îÄ global_principles.md
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/               # Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent1.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent2.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Pydantic data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent1_models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent2_models.py
‚îÇ   ‚îú‚îÄ‚îÄ tools/                # Tool implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_tool.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ custom_tools.py
‚îÇ   ‚îî‚îÄ‚îÄ orchestration/        # Workflow orchestration
‚îÇ       ‚îú‚îÄ‚îÄ pipeline.py
‚îÇ       ‚îî‚îÄ‚îÄ workflow.py
‚îÇ
‚îú‚îÄ‚îÄ tests/                    # Test suites
‚îÇ   ‚îú‚îÄ‚îÄ test_agents/
‚îÇ   ‚îî‚îÄ‚îÄ test_orchestration/
‚îÇ
‚îú‚îÄ‚îÄ config/                   # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml
‚îÇ
‚îú‚îÄ‚îÄ docs/                     # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îî‚îÄ‚îÄ USAGE.md
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## 4. Customizing Agent Architecture

### 4.1 Define Your Agents

**Step 1: Identify Agents**

Think about your workflow and break it into distinct responsibilities:

```python
# Example: Document Processing System
agents = [
    {
        "name": "DocumentExtractor",
        "purpose": "Extract text and metadata from documents",
        "input": "Document file path",
        "output": "ExtractedData model"
    },
    {
        "name": "ContentClassifier",
        "purpose": "Classify document type and content",
        "input": "ExtractedData",
        "output": "ClassificationResult"
    },
    {
        "name": "EntityRecognizer",
        "purpose": "Identify entities (people, places, dates)",
        "input": "ClassificationResult",
        "output": "EntityResult"
    },
    {
        "name": "SummaryGenerator",
        "purpose": "Generate document summary",
        "input": "EntityResult",
        "output": "DocumentSummary"
    }
]
```

**Step 2: Create Data Models**

```python
# src/models/document_models.py
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime

class ExtractedData(BaseModel):
    """Data extracted from document."""
    text: str
    metadata: dict
    page_count: int
    file_type: str
    extracted_at: datetime

class Classification(BaseModel):
    """Document classification."""
    document_type: str  # "invoice", "contract", "report"
    confidence: float
    categories: List[str]
    language: str

class Entity(BaseModel):
    """Recognized entity."""
    text: str
    type: str  # "PERSON", "ORG", "DATE", "MONEY"
    start_pos: int
    end_pos: int
    confidence: float

class EntityResult(BaseModel):
    """Entity recognition result."""
    entities: List[Entity]
    relationships: List[dict]

class DocumentSummary(BaseModel):
    """Document summary."""
    summary: str
    key_points: List[str]
    entities: List[Entity]
    classification: Classification
```

**Step 3: Implement Agents**

```python
# src/agents/document_extractor.py
from .base_agent import CopilotAgent, AgentConfig
from ..models.document_models import ExtractedData
from pathlib import Path

class DocumentExtractorAgent(CopilotAgent):
    """Extract text and metadata from documents."""
    
    def __init__(self):
        config = AgentConfig(
            name="DocumentExtractor",
            model="gpt-4o",
            instructions=self._load_instructions(),
            tools=["file_read", "ocr", "pdf_parse"],
            timeout=120
        )
        super().__init__(config)
    
    async def process(self, input_data: Path) -> ExtractedData:
        """
        Extract data from document.
        
        Args:
            input_data: Path to document file
        
        Returns:
            Extracted data
        """
        # Read file
        file_content = self._read_file(input_data)
        
        # Call Copilot API
        prompt = f"""
        Extract all text and metadata from this document.
        
        File: {input_data.name}
        Type: {input_data.suffix}
        
        Return JSON matching ExtractedData schema.
        """
        
        context = {
            "file_path": str(input_data),
            "file_content": file_content
        }
        
        response = await self._call_copilot_api(prompt, context)
        
        return ExtractedData.model_validate_json(response)
```

### 4.2 Create Agent Instructions

```markdown
# .github/agents/document_extractor.md

# Document Extractor Agent

**Purpose:** Extract text and metadata from various document formats

**Model:** gpt-4o

## Instructions

You are an expert at extracting structured data from documents.

## Available Tools

1. **file_read**: Read file contents
2. **ocr**: Perform OCR on images/scanned documents
3. **pdf_parse**: Parse PDF structure

## Core Rules (BLOCKING)

1. Return ONLY valid JSON matching ExtractedData schema
2. Preserve all extracted text exactly as it appears
3. Extract ALL metadata fields available

## High Priority Rules

1. Use appropriate tool based on file type:
   - PDF: Use pdf_parse
   - Image: Use ocr
   - Text: Use file_read

2. Extract metadata:
   - Author
   - Created date
   - Modified date
   - Page count
   - Language

## Output Format

```json
{
  "text": "Full extracted text...",
  "metadata": {
    "author": "...",
    "created_date": "...",
    "page_count": 10
  },
  "page_count": 10,
  "file_type": "pdf",
  "extracted_at": "2026-02-14T12:00:00Z"
}
```

## Error Handling

- If OCR fails, mark text as needs_manual_review
- If metadata missing, set to null
- Never guess or fabricate data
```

### 4.3 Set Up Orchestration

```python
# src/orchestration/document_pipeline.py
from ..agents.factory import AgentFactory
from ..models.document_models import DocumentSummary
from pathlib import Path

class DocumentProcessingPipeline:
    """Orchestrate document processing agents."""
    
    async def process(self, document_path: Path) -> DocumentSummary:
        """
        Process document through full pipeline.
        
        Args:
            document_path: Path to document
        
        Returns:
            Complete document summary
        """
        # Stage 1: Extract
        async with AgentFactory.create("document_extractor") as agent:
            extracted = await agent.process(document_path)
        
        # Stage 2: Classify
        async with AgentFactory.create("content_classifier") as agent:
            classification = await agent.process(extracted)
        
        # Stage 3: Recognize entities
        async with AgentFactory.create("entity_recognizer") as agent:
            entities = await agent.process(classification)
        
        # Stage 4: Generate summary
        async with AgentFactory.create("summary_generator") as agent:
            summary = await agent.process(entities)
        
        return summary
```

---

## 5. Adapting to Different Domains

### 5.1 Cloud Infrastructure (AWS Version)

```python
# Adapt SynthForge.AI for AWS instead of Azure

# Changes needed:
# 1. Replace Azure service detection with AWS services
# 2. Update ARM types to AWS resource types
# 3. Change tools from Bing to AWS documentation search
# 4. Update IaC from Bicep to CloudFormation/CDK

# src/models/aws_models.py
class AWSResource(BaseModel):
    """AWS resource detected in diagram."""
    service_name: str  # "EC2", "S3", "Lambda"
    resource_type: str  # "AWS::EC2::Instance"
    region: str
    account_id: Optional[str]
    tags: Dict[str, str]

# .github/agents/aws_vision_agent.md
"""
Detect AWS services from architecture diagrams.

Use AWS Architecture Icons as reference.
Look up resource types in CloudFormation format (AWS::Service::Resource).
"""
```

### 5.2 Kubernetes Deployments

```python
# Adapt for Kubernetes manifest generation

# src/models/k8s_models.py
class KubernetesResource(BaseModel):
    """Kubernetes resource."""
    kind: str  # "Deployment", "Service", "Ingress"
    api_version: str
    metadata: dict
    spec: dict

class K8sManifest(BaseModel):
    """Complete K8s manifest."""
    resources: List[KubernetesResource]
    namespace: str
    labels: Dict[str, str]

# Agents needed:
# 1. DiagramAnalyzer - detect components
# 2. K8sMapper - map to K8s resources
# 3. ManifestGenerator - generate YAML
# 4. HelmChartGenerator - create Helm charts
```

### 5.3 Data Pipeline Generation

```python
# Adapt for data pipeline generation (Airflow, etc.)

# src/models/pipeline_models.py
class DataSource(BaseModel):
    """Data source configuration."""
    type: str  # "postgres", "s3", "api"
    connection: dict
    query: Optional[str]

class DataTransformation(BaseModel):
    """Transformation step."""
    name: str
    type: str  # "filter", "aggregate", "join"
    config: dict

class DataPipeline(BaseModel):
    """Complete data pipeline."""
    name: str
    sources: List[DataSource]
    transformations: List[DataTransformation]
    destination: DataSource
    schedule: str

# Agents needed:
# 1. SourceAnalyzer - identify data sources
# 2. TransformationMapper - map transformations
# 3. PipelineGenerator - generate Airflow DAG
# 4. TestGenerator - create pipeline tests
```

---

## 6. Example Projects

### 6.1 Example: Code Documentation Generator

```
Project: CodeDocGen
Purpose: Automatically generate comprehensive documentation from code

Agents:
1. CodeAnalyzer
   - Input: Repository path
   - Output: CodeStructure (modules, classes, functions)
   
2. APIExtractor
   - Input: CodeStructure
   - Output: APIDefinitions (public APIs, parameters, returns)
   
3. ExampleGenerator
   - Input: APIDefinitions
   - Output: CodeExamples (usage examples for each API)
   
4. DocumentationWriter
   - Input: APIDefinitions + CodeExamples
   - Output: MarkdownDocs (complete documentation)
   
5. DiagramGenerator
   - Input: CodeStructure
   - Output: UMLDiagrams (class diagrams, sequence diagrams)

Workflow:
CodeAnalyzer ‚Üí APIExtractor ‚Üí ExampleGenerator ‚Üí DocumentationWriter
                                               ‚Üò DiagramGenerator
```

### 6.2 Example: Security Audit System

```
Project: SecAudit
Purpose: Automated security auditing of infrastructure and code

Agents:
1. CodeScanner
   - Scan code for vulnerabilities
   - Tools: Semgrep, Bandit, CodeQL
   
2. ConfigAnalyzer
   - Analyze infrastructure configs
   - Check for misconfigurations
   
3. DependencyChecker
   - Check dependencies for known vulnerabilities
   - Tools: Dependabot, Snyk
   
4. ComplianceValidator
   - Validate against compliance standards
   - Standards: SOC2, HIPAA, PCI-DSS
   
5. ReportGenerator
   - Generate comprehensive audit report
   - Prioritize findings by severity

Workflow:
[Code] ‚Üí CodeScanner ‚îÄ‚îê
[Config] ‚Üí ConfigAnalyzer ‚îÄ‚îº‚Üí ReportGenerator
[Deps] ‚Üí DependencyChecker ‚îÄ‚î§
[Standards] ‚Üí ComplianceValidator ‚îÄ‚îò
```

### 6.3 Example: Test Generation System

```
Project: TestGen
Purpose: Generate comprehensive test suites from code

Agents:
1. CodeUnderstanding
   - Analyze code to understand functionality
   - Identify testable units
   
2. TestCaseGenerator
   - Generate test cases for each unit
   - Include edge cases, error cases
   
3. MockGenerator
   - Generate mocks for dependencies
   - Create test fixtures
   
4. TestRunner
   - Execute generated tests
   - Collect coverage metrics
   
5. TestOptimizer
   - Remove redundant tests
   - Improve test quality based on results

Workflow with Feedback:
CodeUnderstanding ‚Üí TestCaseGenerator ‚Üí MockGenerator
                                              ‚Üì
                                         TestRunner
                                              ‚Üì
                                       TestOptimizer
                                              ‚Üì
                                    [If coverage < 80%]
                                              ‚Üì
                                    Back to TestCaseGenerator
```

---

## 7. Scaffolding Tools

### 7.1 Project Generator Script

Create `scripts/scaffold_project.py`:

```python
#!/usr/bin/env python3
"""
Project scaffolding tool for creating new multi-agent projects.
"""

import os
import sys
from pathlib import Path
from typing import List, Dict
import yaml

def prompt_project_info() -> Dict:
    """Prompt user for project information."""
    print("=" * 60)
    print("Multi-Agent Project Scaffolder")
    print("=" * 60)
    
    info = {
        "name": input("\nProject name: "),
        "description": input("Project description: "),
        "domain": input("Domain (e.g., aws, kubernetes, data-pipeline): "),
    }
    
    # Agent configuration
    num_agents = int(input("\nHow many agents? "))
    agents = []
    
    for i in range(num_agents):
        print(f"\n--- Agent {i + 1} ---")
        agent = {
            "name": input("Agent name (PascalCase): "),
            "purpose": input("Agent purpose: "),
            "input_type": input("Input type: "),
            "output_type": input("Output type: "),
        }
        agents.append(agent)
    
    info["agents"] = agents
    
    # Orchestration
    print("\n--- Orchestration ---")
    print("1. Sequential")
    print("2. Parallel")
    print("3. Event-driven")
    print("4. Hierarchical")
    orchestration = input("Choose orchestration pattern (1-4): ")
    
    orchestration_map = {
        "1": "sequential",
        "2": "parallel",
        "3": "event_driven",
        "4": "hierarchical"
    }
    info["orchestration"] = orchestration_map.get(orchestration, "sequential")
    
    return info


def create_project_structure(project_dir: Path):
    """Create project directory structure."""
    dirs = [
        ".github/agents",
        ".github/workflows",
        ".github/copilot",
        "src/agents",
        "src/models",
        "src/tools",
        "src/orchestration",
        "tests/test_agents",
        "tests/test_orchestration",
        "config",
        "docs",
    ]
    
    for dir_path in dirs:
        (project_dir / dir_path).mkdir(parents=True, exist_ok=True)


def generate_agent_file(project_dir: Path, agent: Dict):
    """Generate agent implementation file."""
    agent_name = agent["name"]
    
    # Generate Python file
    agent_file = project_dir / "src" / "agents" / f"{agent_name.lower()}_agent.py"
    
    content = f'''"""
{agent["name"]} Agent - {agent["purpose"]}
"""

from .base_agent import CopilotAgent, AgentConfig
from ..models import {agent["input_type"]}, {agent["output_type"]}

class {agent_name}Agent(CopilotAgent):
    """
    {agent["purpose"]}
    """
    
    def __init__(self):
        config = AgentConfig(
            name="{agent_name}",
            model="gpt-4o",
            instructions=self._load_instructions(),
            tools=["web_search", "file_search"],
            timeout=300
        )
        super().__init__(config)
    
    async def process(self, input_data: {agent["input_type"]}) -> {agent["output_type"]}:
        """
        Process input data.
        
        Args:
            input_data: {agent["input_type"]}
        
        Returns:
            {agent["output_type"]}
        """
        prompt = """
        {agent["purpose"]}
        
        Input: {{input_data}}
        
        Return JSON matching {agent["output_type"]} schema.
        """
        
        response = await self._call_copilot_api(prompt, {{"input": input_data}})
        return {agent["output_type"]}.model_validate_json(response)
'''
    
    agent_file.write_text(content)
    
    # Generate instruction file
    instruction_file = project_dir / ".github" / "agents" / f"{agent_name.lower()}_agent.md"
    
    instruction_content = f'''# {agent_name} Agent

**Purpose:** {agent["purpose"]}

**Model:** gpt-4o

## Instructions

You are responsible for: {agent["purpose"]}

## Available Tools

1. **web_search**: Search documentation and best practices
2. **file_search**: Search in codebase

## Core Rules (BLOCKING)

1. Return ONLY valid JSON matching {agent["output_type"]} schema
2. Preserve all input data
3. Never guess or fabricate information

## High Priority Rules

1. Use tools to research current best practices
2. Cite documentation sources
3. Include confidence levels when uncertain

## Output Format

Return JSON matching {agent["output_type"]} schema.
'''
    
    instruction_file.write_text(instruction_content)


def generate_orchestration(project_dir: Path, info: Dict):
    """Generate orchestration code."""
    pattern = info["orchestration"]
    
    if pattern == "sequential":
        generate_sequential_orchestration(project_dir, info["agents"])
    elif pattern == "parallel":
        generate_parallel_orchestration(project_dir, info["agents"])
    # ... other patterns


def generate_sequential_orchestration(project_dir: Path, agents: List[Dict]):
    """Generate sequential pipeline."""
    workflow_file = project_dir / "src" / "orchestration" / "pipeline.py"
    
    content = '''"""
Sequential pipeline orchestration.
"""

from ..agents.factory import AgentFactory

class Pipeline:
    """Sequential agent pipeline."""
    
    async def run(self, initial_input):
        """Run pipeline."""
        current_data = initial_input
        
'''
    
    for agent in agents:
        content += f'''
        # {agent["name"]}
        async with AgentFactory.create("{agent["name"].lower()}") as agent:
            current_data = await agent.process(current_data)
'''
    
    content += '''
        return current_data
'''
    
    workflow_file.write_text(content)


def main():
    """Main scaffolding function."""
    # Get project info
    info = prompt_project_info()
    
    # Create project directory
    project_dir = Path.cwd() / info["name"]
    if project_dir.exists():
        response = input(f"\n{project_dir} exists. Overwrite? (y/N): ")
        if response.lower() != 'y':
            print("Aborted.")
            return
    
    print(f"\nCreating project in {project_dir}...")
    
    # Create structure
    create_project_structure(project_dir)
    
    # Generate agent files
    for agent in info["agents"]:
        print(f"Generating {agent['name']} agent...")
        generate_agent_file(project_dir, agent)
    
    # Generate orchestration
    print("Generating orchestration...")
    generate_orchestration(project_dir, info)
    
    # Generate README
    print("Generating README...")
    readme_file = project_dir / "README.md"
    readme_file.write_text(f'''# {info["name"]}

{info["description"]}

## Architecture

This project uses a multi-agent architecture with {len(info["agents"])} agents:

{chr(10).join(f"- **{a['name']}**: {a['purpose']}" for a in info["agents"])}

## Orchestration

Pattern: {info["orchestration"]}

## Usage

```bash
# Install dependencies
pip install -r requirements.txt

# Run pipeline
python -m src.orchestration.pipeline <input>
```

## Development

See docs/ for architecture and development guides.
''')
    
    print(f"\n‚úì Project created successfully in {project_dir}")
    print("\nNext steps:")
    print(f"  cd {info['name']}")
    print("  pip install -r requirements.txt")
    print("  # Customize agents and models")
    print("  # Add tests")
    print("  # Update README")


if __name__ == "__main__":
    main()
```

### 7.2 Using the Scaffolder

```bash
# Run the scaffolder
python scripts/scaffold_project.py

# Follow prompts:
Project name: DocumentProcessor
Project description: AI-powered document processing system
Domain: document-processing

How many agents? 4

--- Agent 1 ---
Agent name (PascalCase): DocumentExtractor
Agent purpose: Extract text and metadata from documents
Input type: Path
Output type: ExtractedData

--- Agent 2 ---
Agent name (PascalCase): ContentClassifier
Agent purpose: Classify document type and content
Input type: ExtractedData
Output type: ClassificationResult

# ... etc

--- Orchestration ---
1. Sequential
2. Parallel
3. Event-driven
4. Hierarchical
Choose orchestration pattern (1-4): 1

‚úì Project created successfully in DocumentProcessor

Next steps:
  cd DocumentProcessor
  pip install -r requirements.txt
  # Customize agents and models
  # Add tests
  # Update README
```

---

## 8. Testing Your New Project

### 8.1 Unit Tests for Agents

```python
# tests/test_agents/test_document_extractor.py
import pytest
from pathlib import Path
from src.agents.document_extractor_agent import DocumentExtractorAgent
from src.models import ExtractedData

@pytest.mark.asyncio
async def test_document_extractor_initialization():
    """Test agent can be initialized."""
    async with DocumentExtractorAgent() as agent:
        assert agent is not None
        assert agent.config.name == "DocumentExtractor"


@pytest.mark.asyncio
async def test_document_extractor_process(test_document):
    """Test document extraction."""
    async with DocumentExtractorAgent() as agent:
        result = await agent.process(test_document)
        
        assert isinstance(result, ExtractedData)
        assert len(result.text) > 0
        assert result.metadata is not None


@pytest.fixture
def test_document():
    """Fixture providing test document."""
    return Path("tests/fixtures/sample_document.pdf")
```

### 8.2 Integration Tests

```python
# tests/test_orchestration/test_pipeline.py
import pytest
from pathlib import Path
from src.orchestration.pipeline import Pipeline

@pytest.mark.asyncio
@pytest.mark.integration
async def test_full_pipeline(test_document):
    """Test complete pipeline."""
    pipeline = Pipeline()
    
    result = await pipeline.run(test_document)
    
    assert result is not None
    # Add specific assertions for your output


@pytest.mark.asyncio
async def test_pipeline_error_handling(invalid_document):
    """Test pipeline handles errors gracefully."""
    pipeline = Pipeline()
    
    with pytest.raises(ValueError):
        await pipeline.run(invalid_document)
```

### 8.3 Run Tests

```bash
# Run all tests
pytest tests/ -v

# Run only unit tests
pytest tests/test_agents/ -v

# Run only integration tests
pytest tests/ -m integration -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## Conclusion

You now have a comprehensive guide for creating new multi-agent projects based on SynthForge.AI. The template provides:

1. **Proven Architecture**: Battle-tested patterns from SynthForge.AI
2. **Flexible Design**: Easily adaptable to different domains
3. **Best Practices**: Built-in error handling, typing, monitoring
4. **Quick Start**: Scaffolding tools to generate boilerplate
5. **Testing**: Test patterns and fixtures

**Key Takeaways:**

- Start with the scaffolding tool to generate boilerplate
- Focus on defining clear agent responsibilities
- Use Pydantic for all data models
- Follow the instruction template pattern
- Test early and often

**Resources:**

- `COPILOT_AGENTS_ANALYSIS.md`: Deep architecture analysis
- `COPILOT_AGENTS_CONVERSION_GUIDE.md`: Detailed conversion steps
- `AGENT_ORCHESTRATION_PATTERNS.md`: Orchestration patterns
- SynthForge.AI source code: Reference implementation

Happy building! üöÄ
